-----------------------------------------------------------------------
| PART 6 - DBT PROJECTS                                               |
-----------------------------------------------------------------------

- Project Structure

    - dbt enforces the top-level structure of a dbt project.  Within the directories of the top-level, you
        can organize the project however you want.


    - Every project must have a 'dbt_project.yml' project configuration file.  A project may also include
        the following resources:

        models        # Each model lives in a single file and contains logic that either transforms raw data
                      #   or is an intermediate step in such a transformation

        snapshots     # A way to capture the state of your mutable tables so you can refer to it later

        seeds         # CSV files with static data that can be loaded using dbt

        tests         # SQL queries that you can write to test models and resources in your project

        macros        # Blocks of code you can reuse multiple times

        docs          # Docs for your projects that you can build

        sources       # A way to name and describe data loaded into your warehouse by extract and load tools

        exposures     # A way to define and describe a downstream use of your project

        metrics       # A way for you to define metrics for your project

        analysis      # A way to organize analytical SQL queries in your project



- Project Configuration

    - We can edit the 'dbt_project.yml' project configuration file to set up common configurations:

        name                     # Your projectâ€™s name in snake case
        version                  # Version of your project
        require-dbt-version      # Restrict your project to only work with a range of dbt Core versions
        profile                  # The profile dbt uses to connect to your data platform
        model-paths              # Directories to where your model and source files live
        seed-paths               # Directories to where your seed files live
        test-paths               # Directories to where your test files live
        analysis-paths           # Directories to where your analyses live
        macro-paths              # Directories to where your macros live
        snapshot-paths           # Directories to where your snapshots live
        docs-paths               # Directories to where your docs blocks live
        vars                     # Project variables you want to use for data compilation



- Adding Sources to Your DAG

    - Sources make it possible to describe the data loaded into your warehouse by your Extract and Load tools.
        By declaring these tables as sources in dbt, you can then:

        - Select from source tables in your models using the {{ source() }} function, helping define the lineage
            of your data

        - Test you assumptions about the source data

        - Calculate the freshness of your source data


    - Sources are defined in .yml files nested under a 'sources:' key:

        models/<filename>.yml
        ---------------------------
        version: 2

        sources:
          - name: jaffle_shop
            database: raw  
            schema: jaffle_shop      # 'schema' is same as 'name' by default
            tables:
              - name: orders
              - name: customers


    - Once a source has been defined, it can be referenced from a model using the {{ source() }} function.

        select ...
        from {{ source('jaffle_shop', 'orders') }}
        left join {{ source('jaffle_shop', 'customers') }} using (customer_id)

        # Compiles to the full table name
        select ...
        from raw.jaffle_shop.orders
        left join raw.jaffle_shop.customers using (customer_id)


    - You can also add tests and descriptions to sources:

        models/<filename>.sql
        ---------------------------
        version: 2

        sources:
          - name: jaffle_shop
            description: This is a replica of the Postgres database used by our app
            tables:
              - name: orders
                description: >
                  One record per order. Includes cancelled and deleted orders.
                columns:
                  - name: id
                    description: Primary key of the orders table
                    tests:
                      - unique
                      - not_null
                  - name: status
                    description: Note that the status can change over time

              - name: ...

          - name: ...



- Snapshotting Source Data Freshness

    - With a couple of extra configs, dbt can optionally snapshot the 'freshness' of the data in your source
        tables.  This is useful for understanding if your data pipelines are in a healthy state.


    - To configure sources to snapshot freshness information, add a 'freshness' block to your source and a
        'loaded_at_field' to your table declaration.

        models/<filename>.yml
        ----------------------------
        version: 2

        sources:
          - name: jaffle_shop
            database: raw
            freshness: # default freshness
              warn_after: {count: 12, period: hour}
              error_after: {count: 24, period: hour}
            loaded_at_field: _etl_loaded_at

            tables:
              - name: orders
                freshness: # make this a little more strict
                  warn_after: {count: 6, period: hour}
                  error_after: {count: 12, period: hour}

              - name: customers # this will use the freshness defined above


              - name: product_skus
                freshness: null # do not check freshness for this table


    - To snapshot freshness information for your sources:

        $ dbt source freshness


    - You can also add custom filters to only snapshot freshness on rows that match the filter.



- dbt Models

    - When you execute 'dbt run', you are running a model that will transform your data without it ever 
        leaving your warehouse.  Models are primarily written as a 'select' statement and saved as a '.sql'
        file.  Python models can also be used, which can be useful in data science contexts or when you
        need a specific Python package's functionality.


    - A SQL model is a select statement defined in a .sql file in the 'models' directory.

        - Each .sql file contains one model / select statement
        - The model name is inherited from the filename
        - Models can be nested in subdirectories within the 'models' directory


    - When you execute the 'dbt run' command, dbt will build this model in the data warehouse by wrapping
        it in a 'CREATE VIEW AS' or 'CREATE TABLE AS' statement.


    - The 'ref' function can be used to build dependencies between models.



- Configuring Models

    - Configurations for models can be set either in the 'dbt_project.yml' file or in the model with a 
        {{ config }} block.  Configurations include changing the materialization, changing the schema, or
        adding tags.


    - To add configurations in the project file:

        dbt_project.yml
        ---------------------
        name: jaffle_shop
        config-version: 2
        ...

        models:
          jaffle_shop: # this matches the `name:`` config
            +materialized: view # this applies to all models in the current project
            marts:
              +materialized: table # this applies to all models in the `marts/` directory
              marketing:
                +schema: marketing # this applies to all models in the `marts/marketing/`` directory


    - To add configurations in the model:

        models/customer.yml
        ---------------------
        {{ config(
            materialized="view",
            schema="marketing"
        ) }}

        with customer_orders as ...



- Python Models

    - Python models are supported on Snowflake, Databricks, and BigQuery.  Databricks and BigQuery's Dataproc
        use PySpark as their processing framework.  Snowflake uses Snowpark, which is similar to PySpark.


    - To define a Python model:

        models/my_python_model.py
        -----------------------------
        def model(dbt, session):
            dbt.config(
                materialized = "table",
                packages = ["holidays"]
            )

            orders_df = dbt.ref("stg_orders")
            return orders_df



- Seeds

    - Seeds are CSV files in the 'seeds' directory that can be loaded into the warehouse using the 'dbt seed'
        command.  They can be useful for static data which changes infrequently.

    
    - To add a seed file:

        seeds/country_codes.csv
        ----------------------------
        country_code,country_name
        US,United States
        CA,Canada
        GB,United Kingdom


    - To add the seeds in your target schema in a table named 'country_codes':

        $ dbt seed


    - Now, you can refer to the table that was created:

        select * from {{ ref('country_codes') }}


    - You can add documentation and tests for seeds by declaring properties.



- Snapshots

    - Analyts often need to 'look back in time' at previous data states in their mutable tables.  dbt provides
        a mechanism called 'snapshots' which records changes to a mutable table over time.


    - Snapshots implement type-2 SCDs over mutable source tables.  For an example, imagine that you have an 'orders'
        table where the 'status' field can be overwritten as the order is processed.

        id     status     updated_at
        -------------------------------
        1      pending    2019-01-01

        id     status     updated_at
        -------------------------------
        1      shipped    2019-01-02


    - Here is an example of a snapshot for this table:

        id     status     updated_at     dbt_valid_from     dbt_valid_to
        -------------------------------------------------------------------
        1      pending    2019-01-01     2019-01-01         2019-01-02
        1      shipped    2019-01-02     2019-01-02         null


    - In dbt, snapshots are select statements defined within a 'snapshot' block in a .sql file, typically in the
        'snapshots' directory.

        snapshots/orders_snapshot.sql
        --------------------------------
        {% snapshot orders_snapshot %}

        {{
            config(
              target_database='analytics',
              target_schema='snapshots',
              unique_key='id',

              strategy='timestamp',
              updated_at='updated_at',
            )
        }}

        select * from {{ source('jaffle_shop', 'orders') }}

        {% endsnapshot %}


    - When you run the 'dbt snapshot' command:

        1. On the first run, dbt will create the initial snapshot table.  This will be the result set of your
             select statement, with additional columns 'dbt_valid_from' and 'dbt_valid_to'.  All records will
             have a 'dbt_valid_to = null'.

        2. On subsequent runs, dbt will check if any records have changed or if any new records have been created.
             The 'dbt_valid_to' will be updated for any records that have been changed.  The updated records and
             any new records will be added with 'dbt_valid_to = null'.


    - Snapshots can be referenced in downstream models the same way as referencing models.



- Materializations

    - Materializations are strategies for persisting dbt models in a warehouse.  There are 4 types of
        materializations in dbt:

        - table
        - view
        - incremental
        - ephemeral


    - By default, dbt models are materialized as views.  To override this:

        dbt_project.yml
        --------------------
        models:
          my_project:
            events:
              +materialized: table


    - Alternatively, materializations can be configured directly inside of the model sql files.

        models/events/stg_event_log.sql
        ----------------------------------
        {{ config(materialized='table', sort='timestamp', dist='user_id') }}

        select * from ...


    - When using the 'view' materialization, your model is rebuilt as a view on each run via a
        CREATE VIEW AS statement.

        - Pros: No additional data is stored, so views on top of source data will always have the latest
                  records in them.

        - Cons: Views that perform significant transformation or are stacked on top of each other are
                  slow to query.

        - Advice: Start with views for your models, and only change to another materialization when you
                    notice performance problems.


    - When using the 'table' materialization, your model is rebuilt as a table on each run, via a
        CREATE TABLE AS statement.

        - Pros: Tables are fast to query

        - Cons: Tables can take a long time to rebuild, especially for complex transformations.  New
                  records in underlying source tables are not automatically added to the table.

        - Advice: Use the table materialization for any models being queried by BI tools.  Also use it for
                    slower transformations that are being used by many downstream models.


    - 'incremental' models allow dbt to insert or update records into a table since the last time dbt was 
        run.

        - Pros: You can significantly reduce the build time by just transforming new records.

        - Cons: Incremental models require extra configuration and are an an advanced use of dbt.

        - Advice: Incremental models are best for event-style data.  Use incremental models when your runs
                    are becoming too slow.


    - 'ephemeral' models are not directly built into the database.  Instead, dbt will interpolate the code
        from this model into dependent models as a CTE.

        - Pros: You can still write reusable logic, without adding another item to the warehouse.

        - Cons: You cannot select directly from this model.  Overuse can make queries hard to debug.

        - Advice: Use for very light-weight transformations early in your DAG that are only used for one or
                    two downstream models.



- Incremental Models

    - Incremental models are built as tables in your warehouse.  The first time a model is run, the table is
        built by transforming all rows of source data.  On subsequent runs, dbt transforms only the rows in
        your source data you tell dbt to filter for, inserting them into the target table that has already
        been built.